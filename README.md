# Machine Learning Experiments: Decision Trees, Logistic Regression & SVM 📊

## Overview 📌
This repository contains **three machine learning projects**, focusing on classification techniques and model optimization. The projects explore **decision trees, logistic regression, and support vector machines (SVM)** in different contexts, including **hyperparameter tuning and credit risk prediction**.

## Objectives 🎯
- Optimize **decision tree depth, splitting criteria, and regularization**.
- Analyze the impact of **regularization in logistic regression and SVM**.
- Build a **predictive model for credit risk assessment** using machine learning.

## Projects & Implementations 🛠️
### **1️⃣ Decision Tree Optimization** 🌳
- Comparison of tree depth and **overfitting vs. generalization**.
- Tuning **Gini impurity vs. entropy** as splitting criteria.
- Regularization techniques (**minimum samples per split, pruning**).
- Evaluation using **cross-validation and accuracy metrics**.

### **2️⃣ Logistic Regression & SVM Classification** 📈
- Implementing **logistic regression with L1 and L2 regularization**.
- Optimizing **hyperparameters (C, kernel types for SVM)**.
- Comparing **logistic regression vs. SVM in classification tasks**.
- Using **precision, recall, and F1-score** for model evaluation.

### **3️⃣ Credit Risk Prediction Model** 💳
- Predicting **credit default risk** using financial data.
- Handling **imbalanced data** with undersampling and oversampling.
- Feature engineering for **improving model performance**.
- Evaluating **balanced accuracy** as the key metric.


## Key Insights 🔎
✅ **Decision tree depth significantly affects overfitting**.  
✅ **L2 regularization in logistic regression improves stability**.  
✅ **SVM with RBF kernel performs well on non-linearly separable data**.  
✅ **Balanced accuracy is a better metric for credit risk classification than simple accuracy**.  

## License 📄
This project is licensed under the **MIT License** – see the `LICENSE` file for details.

---
### 🚀 Exploring ML techniques for better predictive modeling!

